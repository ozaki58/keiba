{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd786c-62a2-41da-8dd6-0da4f8686fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "\n",
    "##レースデータを加工する関数\n",
    "def preprocessing(results):\n",
    "    df = results.copy()\n",
    "\n",
    "    # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "    df ['着順']=pd.to_numeric(df['着順'], errors='coerce')\n",
    "    df.dropna(subset=['着順'], inplace=True)\n",
    "    df[\"着順\"] = df[\"着順\"].astype(int)\n",
    "\n",
    "    # 性齢を性と年齢に分ける\n",
    "    df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "    df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "    # 馬体重を体重と体重変化に分ける\n",
    "    df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0].astype(int)\n",
    "    df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1].astype(int)\n",
    "\n",
    "    # データをint, floatに変換\n",
    "    df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "\n",
    "    # 不要な列を削除\n",
    "    df.drop([\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\",\"馬名\",\"騎手\"], axis=1, inplace=True)\n",
    "    \n",
    "    df['date']=pd.to_datetime(df['date'],format='%Y年%m月%d日')\n",
    "    return df\n",
    "\n",
    "##データをテストデータと訓練データに分ける\n",
    "def split_data(df,test_size=0.3):\n",
    "    sorted_id_list=df.sort_values('date').index.unique()\n",
    "    train_id_list=sorted_id_list[:round(len(sorted_id_list)*(1-test_size))]\n",
    "    test_id_list=sorted_id_list[round(len(sorted_id_list)*(1-test_size)):]\n",
    "    \n",
    "    train=df.loc[train_id_list].drop(['date'],axis=1)\n",
    "    test=df.loc[test_id_list].drop(['date'],axis=1)\n",
    "    return train,test\n",
    "\n",
    "##カテゴライズ関数\n",
    "\n",
    "def categoraize(results,peds):\n",
    "    df=results.copy()\n",
    "\n",
    "    ##horse_idとpedsを０からはじまる整数型に変更\n",
    "    df['horse_id'] = LabelEncoder().fit_transform(df['horse_id'])\n",
    "    for i in peds.columns:\n",
    "        df[i] = LabelEncoder().fit_transform(df[i])\n",
    "\n",
    "\n",
    "    #ダミー変数化\n",
    "    results_d = pd.get_dummies(df)\n",
    "\n",
    "    #horse_idとpedsをpandasのcategory型に変換\n",
    "    results_d['horse_id'] = results_d['horse_id'].astype('category')\n",
    "    for i in peds.columns:\n",
    "        results_d[i] = results_d[i].astype('category')\n",
    "    return results_d\n",
    "\n",
    "##実際にシュミレーションが賭けた総数をキーにして、値に回収率を入れる\n",
    "def gain(return_func, X, n_samples=100, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        threshold = 1 * i / n_samples + min_threshold * (1-(i/n_samples))\n",
    "        n_bets, money = return_func(X, threshold)\n",
    "        if n_bets > 50:\n",
    "            gain[n_bets] = (n_bets*100 + money) / (n_bets*100)\n",
    "    return pd.Series(gain)\n",
    "\n",
    "##馬ごとの過去成績をデータを操作するクラス\n",
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"日付\"])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        #賞金のNaNを0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "    \n",
    "        self.horse_results = df\n",
    "        \n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.loc[horse_id_list]\n",
    "        \n",
    "        #過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].\\\n",
    "                sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be >0')\n",
    "            \n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        return average.rename(columns={'着順':'着順_{}R'.format(n_samples), '賞金':'賞金_{}R'.format(n_samples)})\n",
    "    \n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date']==date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, n_samples), left_on='horse_id',\n",
    "                             right_index=True, how='left')\n",
    "        return merged_df\n",
    "    \n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tqdm(date_list)])\n",
    "        return merged_df\n",
    "\n",
    "    ##レースの払い戻しデータを操作をするクラス\n",
    "class Odds:\n",
    "    def __init__(self,return_tables):\n",
    "        self.return_tables=return_tables\n",
    "        \n",
    "    ##複勝のオッズを取り出す\n",
    "    def hukusho(self):\n",
    "        lists=[\"賭け方\",\"馬番\",\"オッズ\",\"人気\"]\n",
    "        self.return_tables.columns=lists\n",
    "        self.hukusho_tables= self.return_tables[self.return_tables.賭け方==\"複勝\"]\n",
    "        self.hukusho_tables.drop(columns=[\"人気\",\"賭け方\"],inplace=True)\n",
    "        \n",
    "        num=self.hukusho_tables[\"馬番\"].str.split('br',expand=True)\n",
    "        odds=self.hukusho_tables[\"オッズ\"].str.split('br',expand=True)\n",
    "        num.columns=[\"wins1\",\"wins2\",\"wins3\"]\n",
    "        odds.columns=[\"wins1_odds\",\"wins2_odds\",\"wins3_odds\"]\n",
    "        hukusho=pd.concat([num,odds],axis=1)\n",
    "        for column in hukusho.columns:\n",
    "            hukusho[column] = hukusho[column].str.replace(',', '')\n",
    "        return  hukusho.fillna(0).astype(int)\n",
    "    \n",
    "    ##単勝のオッズを取り出す\n",
    "    def tansho(self):\n",
    "        tansho_tables=self.return_tables[self.return_tables.賭け方==\"単勝\"]\n",
    "        tansho_tables.drop(columns=[\"人気\",\"賭け方\"],inplace=True)\n",
    "        tansho_tables.columns=['win','オッズ']\n",
    "        for column in tansho_tables.columns:\n",
    "            tansho_tables[column] = pd.to_numeric(tansho_tables[column], errors='coerce')\n",
    "            \n",
    "    \n",
    "        return tansho_tables\n",
    "    \n",
    "    ##モデルについてのクラス\n",
    "class Model:\n",
    "    def __init__(self,model,return_tables):\n",
    "        self.model=model\n",
    "        self.hukusho=Odds(return_tables).hukusho()\n",
    "        self.tansho=Odds(return_tables).tansho()\n",
    "        \n",
    "    ##モデルが3着以内にくると予測する確率をだす\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)[:,1]\n",
    "    \n",
    "    ##確率が0.5以上のものを1、それ以外を0とする\n",
    "    def predict_value(self,X,threshold=0.5):\n",
    "        \n",
    "        y_pred=self.predict_proba(X)\n",
    "        for index,element in enumerate(y_pred) :\n",
    "            if   element>threshold:\n",
    "                y_pred[index]=1\n",
    "            else:\n",
    "                y_pred[index]=0 \n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    ##aucscoreを表示\n",
    "    def score(self, y_true, X):\n",
    "        return roc_auc_score(y_true, self.predict_proba(X))\n",
    "    \n",
    "　　##特徴量の重要度を表示\n",
    "    def feature_importance(self, X, n_display=20):\n",
    "        importances = pd.DataFrame({\"features\": X.columns, \n",
    "                                    \"importance\": self.model.feature_importances_})\n",
    "        return importances.sort_values(\"importance\", ascending=False)[:n_display]\n",
    "\n",
    "    ##predの値が１の馬番を取り出す \n",
    "    def pred_table(self,X,threshold=0.5):\n",
    "        pred_table=X.copy()[[\"馬番\"]]\n",
    "        pred_table[\"pred\"]=self.predict_value(X,threshold)\n",
    "        pred_table=pred_table[pred_table.pred==1]\n",
    "        \n",
    "        return pred_table\n",
    "    \n",
    "  ##複勝で購入した場合の払い戻し額の合計を算出\n",
    "    def simulate(self,X,threshold=0.5):\n",
    "        pred_table = self.pred_table(X,threshold)\n",
    "        df=self.hukusho\n",
    "        df=df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        n_bets=len(pred_table)\n",
    "        money=-100*n_bets\n",
    "        for i in range(3):  \n",
    "             money += df[df['wins{}'.format(i+1)]==df['馬番']]['wins{}_odds'.format(i+1)].sum()\n",
    "        return n_bets,money\n",
    "    \n",
    "    ##単勝で購入した場合の払い戻し額の合計を算出\n",
    "    def tansho_simulate(self,X,threshold=0.5):\n",
    "        pred_table = self.pred_table(X,threshold)\n",
    "        df=self.tansho\n",
    "        df=df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        n_bets=len(pred_table)\n",
    "        money=-100*n_bets\n",
    "        money += df[df['win']==df['馬番']]['オッズ'].sum()\n",
    "        return n_bets,money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b339db-35f0-46d5-9f7e-cbf2748f3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "  \n",
    "    race_results = pre_race_results.copy() \n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            race_results[race_id] = pd.read_html(url)[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return race_results\n",
    "\n",
    "#レースIDのリストを作る\n",
    "race_id_list = []\n",
    "for kai in range(1, 6, 1):\n",
    "    for day in range(1, 13, 1):\n",
    "         for r in range(1, 13, 1):\n",
    "            race_id = \"201908\" + str(kai).zfill(2) +\\\n",
    "            str(day).zfill(2) + str(r).zfill(2)\n",
    "            race_id_list.append(race_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c3da8-964d-4c4e-b9ed-9c32d1864141",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = scrape_race_results(race_id_list)\n",
    "for key in test3: \n",
    "    test3[key].index = [key] * len(test3[key])\n",
    "results = pd.concat([test3[key] for key in test3], sort=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75a258-bbb8-4ef8-bcd8-49b0534b99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import datetime\n",
    "def scrape_race_info(race_id_list):\n",
    "  \n",
    "    race_infos={}\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url=\"https://db.netkeiba.com/race/\" + race_id\n",
    "            html =requests.get(url)\n",
    "            html.encoding=\"EUC-JP\"\n",
    "            soup=BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "            texts=soup.find('div',attrs={'class':'data_intro'}).find_all('p')[0].text+\\\n",
    "                soup.find('div',attrs={'class':'data_intro'}).find_all('p')[1].text\n",
    "            info=re.findall(r'\\w+',texts)\n",
    "            info_dict={}\n",
    "            for text in info:\n",
    "                if text in ['芝','ダート']:\n",
    "                    info_dict['race_type'] = text\n",
    "                if '障' in text:\n",
    "                    info_dict['race_type']= '障害'\n",
    "                if 'm'in text:\n",
    "                    info_dict['course_len']=int(re.findall(r'\\d+',text)[0])\n",
    "                if text in ['良','慎重','重','不良']:\n",
    "                    info_dict['ground_state']=text\n",
    "                if text in ['曇','晴','雨','小雨','小雪','雪']:\n",
    "                    info_dict['weather']=text\n",
    "                if '年'in text:\n",
    "                    info_dict['date']=text\n",
    "\n",
    "            race_infos[race_id]=info_dict\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "             continue\n",
    "        except:\n",
    "               break\n",
    "    return race_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c6fe6-f678-4f46-822c-1aefd64dd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = results.index.unique()\n",
    "race_infos = scrape_race_info(race_id_list)\n",
    "\n",
    "#DataFrame型にする\n",
    "race_infos = pd.DataFrame(race_infos).T\n",
    "\n",
    "#resultsに結合\n",
    "results_addinfo = results.merge(race_infos, left_index=True, right_index=True, how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70efe160-ab33-49eb-8662-74c8f1c21f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_race_add(race_id_list):   \n",
    "    #race_idをkeyにしてDataFrame型を格納\n",
    "    race_results = {}\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            time.sleep(1)\n",
    "\t    #メインとなるテーブルデータを取得\n",
    "            df = pd.read_html(url)[0]\n",
    "            \n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            #馬ID、騎手IDをスクレイピング\n",
    "            horse_id_list = []\n",
    "            horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "            )\n",
    "            for a in horse_a_list:\n",
    "                horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                horse_id_list.append(horse_id[0])\n",
    "            jockey_id_list = []\n",
    "            jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "            )\n",
    "            for a in jockey_a_list:\n",
    "                jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                jockey_id_list.append(jockey_id[0])\n",
    "            df[\"horse_id\"] = horse_id_list\n",
    "            df[\"jockey_id\"] = jockey_id_list\n",
    "            \n",
    "            race_results[race_id] = df\n",
    "\t#存在しないrace_idを飛ばす\n",
    "        except IndexError:\n",
    "            continue\n",
    "\t#wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\t#Jupyterで停止ボタンを押した時の対処    \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    return race_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b121fc-2c9e-4de1-aa1a-51e5c8f938ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#スクレイピング実行\n",
    "race_results = scrape_race_results(race_id_list)\n",
    "\n",
    "#indexをrace_idにする\n",
    "for key in race_results:\n",
    "    race_results[key].index = [key] * len(race_results[key])\n",
    "\n",
    "#pd.DataFrame型にして一つのデータにまとめる\n",
    "race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "#race_infosをmerge\n",
    "results = race_results_df.merge(race_infos, left_index=True,\n",
    "    right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97723-c0ff-46b1-8a8e-b238d53e6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horse_results(horse_id_list):\n",
    "    #horse_idをkeyにしてDataFrame型を格納\n",
    "    horse_results = {}\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            df = pd.read_html(url)[3]\n",
    "\t\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(url)[4]\n",
    "            horse_results[horse_id] = df\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    return horse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde417f-0b6d-45eb-bc03-a8c451116ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_results=scrape_horse_results(horse_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391a040-5413-4ae8-9743-1d569babd6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in horse_results:\n",
    "    horse_results[key].index = [key] * len(horse_results[key])\n",
    "    \n",
    "#一つのDataFrame型のデータにまとめる。\n",
    "horse_results = pd.concat([horse_results[key] for key in horse_results])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e8ee8-a1c7-4ebb-95a7-5f8d59649a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def scrape_return_tables(race_id_list, pre_return_tables={}):\n",
    "    return_tables = pre_return_tables\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            \n",
    "            #普通にスクレイピングすると複勝やワイドなどが区切られないで繋がってしまう。\n",
    "            #そのため、改行コードを文字列brに変換して後でsplitする\n",
    "            f = urlopen(url)\n",
    "            html = f.read()\n",
    "            html = html.replace(b'<br />', b'br')\n",
    "            dfs = pd.read_html(html)\n",
    "\n",
    "            #dfsの1番目に単勝〜馬連、2番目にワイド〜三連単がある\n",
    "            df = pd.concat([dfs[1], dfs[2]])\n",
    "\n",
    "            df.index = [race_id] * len(df)\n",
    "            return_tables[race_id] = df\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e: #捕捉できるエラーは原因がわかるようにprintしてからbreak\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return return_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a3b4c-b495-4819-90d6-2568c51171f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tables=scrape_return_tables(rece_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c1145-67d0-4b4c-b857-1c5d04286dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in return_tables.keys():\n",
    "    return_tables[key].index = [key]*len(return_tables[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b14b9-76a2-45bf-b966-11397be836b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tables = pd.concat([return_tables[key] for key in return_tables.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1c477-c144-4221-a0e8-d2f897000152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_peds(horse_id_list, pre_peds = {}):\n",
    "    peds = pre_peds\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/horse/ped/\" + horse_id\n",
    "            df = pd.read_html(url)[0]\n",
    "\n",
    "            #重複を削除して1列のSeries型データに直す\n",
    "            generations = {}\n",
    "            for i in reversed(range(5)):\n",
    "                generations[i] = df[i]\n",
    "                df.drop([i], axis=1, inplace=True)\n",
    "                df = df.drop_duplicates()\n",
    "            ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "\n",
    "            peds[horse_id] = ped.reset_index(drop=True)\n",
    "            time.sleep(1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return peds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33994c55-ad5a-47eb-8a1c-e56256c219d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peds=scrape_peds(horse_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a47f2-9a2a-40ab-9f01-66b1ea22ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in peds:\n",
    "    peds[key].index = [key]*len(peds[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c9691-b210-4e29-8652-a0c90f2310f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "peds = pd.concat([peds[key] for key in peds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605269bb-f79b-47bf-b222-c674a19afd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=peds[0].copy()\n",
    "\n",
    "peds= pd.DataFrame(index=horse_id_list)\n",
    "for e in tqdm(horse_id_list):\n",
    "    for i  in range(62):\n",
    "        peds3.loc[e, [i]]=a[e][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a70126-714f-42e3-86c3-97e45d73bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "peds=peds.add_prefix('peds_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5360e9-bc42-42f0-8272-d48cc37de022",
   "metadata": {},
   "source": [
    "##　予測開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17106ec7-73e8-49aa-a810-6aca4a2ac92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p=preprocessing(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a10b8-a96f-4ff5-a509-f574b0372803",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr=HorseResults(horse_results)\n",
    "results_m=hr.merge_all(results_p,n_samples=5)\n",
    "results_m=hr.merge_all(results_m,n_samples=9)\n",
    "results_m=hr.merge_all(results_m,n_samples='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff1811-2e6a-40db-8a87-5c565be2786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m=results_m.merge(peds,left_on='horse_id', right_index=True,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffc26b-25eb-421c-bb86-238abbf5447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d=categoraize(results_m,peds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c70aa-f954-4f00-8fca-eb3036525626",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d['rank']=results_d['着順'].map(lambda x: 1 if x<4 else 0)\n",
    "results_r=results_d.drop(['着順'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf83d4a-4f22-4ceb-9660-fd7ea8c13658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train, test = split_data(results_r)\n",
    "X_train = train.drop(['rank'], axis=1)\n",
    "y_train = train['rank']\n",
    "X_test = test.drop(['rank'], axis=1)\n",
    "y_test = test['rank']\n",
    "X_train.drop(['単勝','人気'],axis=1,inplace=True)\n",
    "X_test.drop(['単勝','人気'],axis=1,inplace=True)\n",
    "params = {\n",
    "    \"num_leaves\": 4,\n",
    "    \"n_estimators\": 80,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    \"random_state\": 100,\n",
    "}\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "lgb_clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555eeeb-00ff-40aa-9661-96448271ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "me=Model(lgb_clf,return_tables)\n",
    "tansho=gain(me.tansho_simulate,X_test)\n",
    "hukusho=gain(me.simulate,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd474b-3b39-4ccc-a8bb-2683a3bb5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "tansho.rename('tansho').plot(legend=True)\n",
    "hukusho.rename('hukusho').plot(legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81ace4-3cac-4c2a-b8aa-1108e68e37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "me.feature_importance(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa692ec9-263e-445a-ad7d-42d775ce8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "me.score(y_test,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e451d20-6c9a-4923-a167-6dedf8c7bd86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
